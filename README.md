# pytorch-wrapping-multi-dataloaders

Taking a batch from each dataloader and combine these batches in some way into a large one, mimicing the `for ... in ...` interface.
